# Problem Setup

This chapter includes all the definitions, symbols, and operatings frequently used in the RLHF process.

## NLP Definitions

* **Prompt ($x$)**: The input text given to a language model to generate a response or completion.

* **Completion ($y$)**: The output text generated by a language model in response to a prompt.

* **Chosen Completion ($y_c$)**: The completion that is selected or preferred over other alternatives, often denoted as $y_{chosen}$.

* **Preference Relation ($\succ$)**: A symbol indicating that one completion is preferred over another, e.g., $y_{chosen} \succ y_{rejected}$.

* **Policy ($\pi$)**: A probability distribution over possible completions, parameterized by $\theta$: $\pi_\theta(y|x)$.

## RL Definitions

* **Reward ($r$)**: A scalar value indicating the desirability of an action or state, typically denoted as $r$.

* **Action ($a$)**: A decision or move made by an agent in an environment, often represented as $a \in A$, where $A$ is the set of possible actions.

* **State ($s$)**: The current configuration or situation of the environment, usually denoted as $s \in S$, where $S$ is the state space.

* **Policy ($\pi$)**: In RL, a policy is a strategy or rule that the agent follows to decide which action to take in a given state: $\pi(a|s)$.

* **Value Function ($V$)**: A function that estimates the expected cumulative reward from a given state: $V(s) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s]$.

* **Q-Function ($Q$)**: A function that estimates the expected cumulative reward from taking a specific action in a given state: $Q(s,a) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, a_0 = a]$.

* **Expectation of Reward Optimization**: The primary goal in RL, which involves maximizing the expected cumulative reward:

  $\max_{\theta} \mathbb{E}_{s \sim \rho_\pi, a \sim \pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]$

  where $\rho_\pi$ is the state distribution under policy $\pi$, and $\gamma$ is the discount factor.
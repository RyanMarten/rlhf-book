# Problem Setup

This chapter includes all the definitions, symbols, and operatings frequently used in the RLHF process.

## ML Definitions

* **Kullback-Leibler (KL) distance ($D_{KL}(P || Q)$)**, also known as KL divergence, is a measure of the difference between two probability distributions. 
For discrete probability distributions $P$ and $Q$ defined on the same probability space $\mathcal{X}$, the KL distance from $Q$ to $P$ is defined as:

$$ D_{KL}(P || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) $$

Key properties of KL distance:

Non-negative: $D_{KL}(P || Q) \geq 0$
Zero if and only if $P = Q$
Asymmetric: $D_{KL}(P || Q) \neq D_{KL}(Q || P)$

In RLHF, KL distance is often used in the objective function to ensure that the learned policy doesn't deviate too far from the initial policy, helping to maintain stable learning and prevent drastic changes in behavior.
Normal practice just uses the forward KL distance.


## NLP Definitions

* **Prompt ($x$)**: The input text given to a language model to generate a response or completion.

* **Completion ($y$)**: The output text generated by a language model in response to a prompt. Often the completion is denoted as $y|x$.

* **Chosen Completion ($y_c$)**: The completion that is selected or preferred over other alternatives, often denoted as $y_{chosen}$.

* **Preference Relation ($\succ$)**: A symbol indicating that one completion is preferred over another, e.g., $y_{chosen} \succ y_{rejected}$.

* **Policy ($\pi$)**: A probability distribution over possible completions, parameterized by $\theta$: $\pi_\theta(y|x)$.

## RL Definitions

* **Reward ($r$)**: A scalar value indicating the desirability of an action or state, typically denoted as $r$.

* **Action ($a$)**: A decision or move made by an agent in an environment, often represented as $a \in A$, where $A$ is the set of possible actions.

* **State ($s$)**: The current configuration or situation of the environment, usually denoted as $s \in S$, where $S$ is the state space.

* **Policy ($\pi$)**: In RL, a policy is a strategy or rule that the agent follows to decide which action to take in a given state: $\pi(a|s)$.

* **Value Function ($V$)**: A function that estimates the expected cumulative reward from a given state: $V(s) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s]$.

* **Q-Function ($Q$)**: A function that estimates the expected cumulative reward from taking a specific action in a given state: $Q(s,a) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, a_0 = a]$.

* **Expectation of Reward Optimization**: The primary goal in RL, which involves maximizing the expected cumulative reward:

  $\max_{\theta} \mathbb{E}_{s \sim \rho_\pi, a \sim \pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]$

  where $\rho_\pi$ is the state distribution under policy $\pi$, and $\gamma$ is the discount factor.